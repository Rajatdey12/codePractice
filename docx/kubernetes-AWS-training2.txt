CODE COMMIT -
AWS feature to store private repositories - like github or lab.


create repos in codeCommit, then connect to the host ec2 & create ssh keys for the codeCommit

create a directory to store the ssh keys -
mkdir ~/.ssh && cd ~/.ssh

to generate new key pair -
ssh-keygen -t rsa

command to update permission for SSH keys -
chmod 600 ~/.ssh/id_rsa

command to upload the public key and attach it to the user -
aws iam upload-ssh-public-key \
--user-name gitUser \
--ssh-public-key-body file://~/.ssh/id_rsa.pub

save the key to a public var -
KEYID=$(aws iam list-ssh-public-keys --user-name gitUser | jq -r '.[] | .[] | .SSHPublicKeyId') && echo $KEYID

create an SSH config file to associate public and private keys -
cat <<EOF > ~/.ssh/config
Host git-codecommit.*.amazonaws.com
  User ${KEYID}
  IdentityFile ~/.ssh/id_rsa
EOF
chmod 700 ~/.ssh/config

ssh into codeCommit -
ssh git-codecommit.us-west-2.amazonaws.com

CREATE A DOCKER IMAGE WITH CODE PIPELINE -

git config --global user.email "rajdeyrail@gmail.com"
git config --global user.name "Rajat Dey"
git config --global init.defaultBranch main

clone eks-example in home dir -
cd ~ && git clone ssh://$KEYID@git-codecommit.us-west-2.amazonaws.com/v1/repos/eks-example

demo Dockerfile -
FROM 012345678901.dkr.ecr.us-west-2.amazonaws.com/awstc:eks-ilt-lab3
RUN rm /etc/nginx/conf.d/*
ADD src/hello.conf /etc/nginx/conf.d/
ADD src/index.html /usr/share/nginx/html/

git add .
git commit -am "Initial commit"
git push

goto AWS codeCommit - edit the repo, select main branch, in the change detection options choose CodePipeline, in save pipeline changes click save.
goto AWS ECR - check the tagged image of eks-example.


to view the repo details in ECR -
aws ecr describe-repositories | jq '.repositories[] | select( .repositoryName == "eks-example")'

save repo to a shell var -
REPO_URI=$(aws ecr describe-repositories | jq -r '.repositories[] | select( .repositoryName == "eks-example") | .repositoryUri')

kubernets manifest file -
cat << EOF > namespaces/eks-example.yaml
apiVersion: v1
kind: Namespace
metadata:
  labels:
    name: eks-example
  name: eks-example
EOF

deployment manifest fle -
cat << EOF > workloads/eks-example-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: eks-example
  namespace: eks-example
  labels:
    app: eks-example
  annotations:
    flux.weave.works/automated: "true"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: eks-example
  template:
    metadata:
      labels:
        app: eks-example
    spec:
      containers:
      - name: eks-example
        image: $REPO_URI
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /
            port: http
        readinessProbe:
          httpGet:
            path: /
            port: http
EOF


https://docs.aws.amazon.com/eks/latest/userguide/service_IAM_role.html
https://aws.amazon.com/ec2/pricing/on-demand/
https://aws.amazon.com/ec2/spot/instance-advisor/

hpa - horizontal pod autoscalar - enables autoscaling in kubs (Horizontal)
kubectl autoscale deployment myapp 

AWS XRay | yagger - log tracing for apps.
trace - full request trace
segment - part of request layer
sub-segment - sub part of request layer


service manifest file - 
cat << EOF > workloads/eks-example-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: eks-example
  namespace: eks-example
  labels:
    app: eks-example
spec:
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app: eks-example
EOF

display all the namespaces, pods, deployments, services for all namesapces -
kubectl get namespaces,services,deployments,pods --all-namespaces

command to find load balanced IP -
kubectl describe service eks-example -n eks-example

command to display & follow weave flux logs  -
kubectl get events --sort-by=.metadata.creationTimestamp -n flux-system --watch
kubectl get events

delete the kubs deployment command -
kubectl delete deployment eks-example -n eks-example && kubectl get events -n eks-example --watch

kustomization controller restores the cluster and pods will be created.

LOG COLLECTION & ANALYSIS -
create amazon kinesis data firehose and configure fluent bit DaemonSet -

export FIREHOSE_ROLE_ARN=FirehoseRoleArn S3_BUCKET_ARN=StreamBucketArn

create a firehose delivery stream -
aws firehose create-delivery-stream --delivery-stream-name eks-stream --delivery-stream-type DirectPut --s3-destination-configuration RoleARN=${FIREHOSE_ROLE_ARN},BucketARN=${S3_BUCKET_ARN},Prefix=eks/

command to create fluent bit namespace -
kubectl create namespace fb

create clusterRole & clusterRoleBinding -
kubectl apply -f ~/scripts/task2/eks-fluent-bit-daemonset-rbac.yaml

create configMap to define the fluent bit log parsing & routing parameters -
kubectl apply -f ~/scripts/task2/eks-fluent-bit-configmap.yaml

create fluent bit kubs daemonset -
kubectl apply -f ~/scripts/task2/eks-fluent-bit-daemonset.yaml

check the status of fluent bit DaemonSet creation -
kubectl get daemonset fluentbit -n fb

display the fluentBit Daemon set logs -
kubectl logs ds/fluentbit -n fb




